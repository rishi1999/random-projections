{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Projections\n",
    "\n",
    "### Rishi Advani, Madison Crim, Sean O'Hagan\n",
    "\n",
    "#### July 10, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenfaces\n",
    "\n",
    "As an application of our work on matrix decomposition and random projection, we decided to use the deterministic and randomized variants of the SVD on a large, real-life dataset. We chose to use the [Labeled Faces in the Wild (LFW) dataset](http://vis-www.cs.umass.edu/lfw/), which consists of ~$8000$ RGB images of dimensions $250\\times 250$. It's computationally infeasible for us to use the whole dataset, so we load a subset (1000 images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Let's look at some of the faces in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "![original faces](images/2020-07-10/original_image_grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can flatten each image to represent it as vector of length $250\\cdot 250 \\cdot 3 = 187500$. This yields a data matrix $A$ of size $(187500,1000)$, which we may now manipulate. After computing the singular value decomposition,\n",
    "$$A = U \\Sigma V^* \\,,$$\n",
    "the columns of $U$ represent an orthonormal basis for the span of the original data, ordered in accordance to their corresponding singular values from greatest to least. Thus, we may truncate $U$ and use the first $k$ columns as a basis for a rank $k$ approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deterministic eigenfaces](images/2020-07-10/det_eigenfaces_grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to deterministic SVD, we may utilize randomized SVD for this task as well. As a refresher, given a desired rank $k$ and an oversampling parameter $p$, we compute a randomized SVD in the following way:\n",
    "\n",
    "- Define $l=k+p$.\n",
    "- Let $\\Omega \\in \\mathbb{R}^{n\\times l}$, with $\\Omega_{ij} \\sim N(0,1)$\n",
    "- Define $Y = A \\Omega$\n",
    "- Compute an orthonormal basis for the columns of $Y$ (i.e. with $QR$ decomposition)\n",
    "- Define $B = Q^* A$. Note $B$ has dimension $l\\times n$\n",
    "- Compute the SVD of B, $B=\\tilde{U}\\Sigma V^*$, and let $U = Q\\tilde{U}$, yielding $A\\approx U\\Sigma V^*$\n",
    "\n",
    "Below are the columns of $U$ obtained using randomized SVD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "![randomized eigenfaces](images/2020-07-10/rand_eigenfaces_grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a basis, we may represent arbitrary vectors in our image space as linear combinations of basis vectors. To do this, we solve the linear system\n",
    "$$A\\vec{x}=\\vec{b}-m \\,,$$\n",
    "where $A$ denotes the matrix of which our basis vectors are columns, $\\vec{b}$ denotes our specific image, and $m$ denotes the (column) mean of the dataset. Since we know our basis is orthonormal, we can use a shortcut, computing the coefficients $\\vec{x}$ as \n",
    "$$\\vec{x} = (\\vec{b}-m)A \\,.$$\n",
    "\n",
    "Below are examples of approximating images at a certain rank after a change of basis. The reason why this technique is useful is that any arbitrary image thought to be similar to those in the dataset may be represented fairly well using this basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "![reconstructed image grid](images/2020-07-10/reconstructed_image_grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reconstructed sean grid](images/2020-07-10/reconstructed_sean_grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enjoy the follow animations:\n",
    "\n",
    "Aaron Eckhart | Sean | President Obama | Ellie Goulding\n",
    "- | - | - | -\n",
    "![Guy](../examples/eigenfaces/guy250.png) | ![Sean](../examples/eigenfaces/sean250.png) | ![Pres](../examples/eigenfaces/president_obama250.png) | ![Ellie](../examples/eigenfaces/ellie250.png)\n",
    "![Anim](images/2020-07-10/anim/guy.gif) | ![Anim](images/2020-07-10/anim/sean.gif) | ![Anim](images/2020-07-10/anim/pres.gif) | ![Anim](images/2020-07-10/anim/ellie.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get better results?\n",
    "\n",
    "- Use more data (and more diverse data)\n",
    "- Train on data that is representative of the real data that the algorithm will be used on\n",
    "- Standardize the position and scale of the face within the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform an analysis on the cost and accuracy of these methods, as the rank $k$ of the approximation varies. We see an inverse relationship between these quantities, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![performance graph 1](images/2020-07-10/performance_relative_to_det.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since the randomized SVD is a random process, we may sample it and obtain statistical information. Again, due to concentration of measure, note the extremely low standard deviation in the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "![performance graph 2](images/2020-07-10/performance_relative_to_orig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least-Squares Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, when trying to solve a linear system of equations $Ax=b$, there may not be an exact solution. One way to find an 'almost solution' is to find the vector $x'$ such that $\\lVert Ax'-b \\rVert_2$ is minimized, where $A$ is a full rank $m \\times n$ matrix such that $m\\geq n$. This least squares solution which works by minimizing the sum of the squares of the differences between the entries of $Ax'$ and $b$ and thus solving the solution $Ax=b$ as closely as possible.\n",
    "\n",
    "If the system has an exact solution, $x'$ is precisely the value of $x$ that solves the system. The reasoning is as follows:\n",
    "\n",
    "The system is solvable, so there is a vector $x$, such that $Ax=b$. Equivalently, there is a vector $x$ such that $Ax-b = 0$. Then, we have $\\lVert Ax-b \\rVert_2 = 0$. The L2 norm is nonnegative for all its inputs by definition, so the smallest value it can take is $0$. Thus, $x$ is the vector that minimizes the expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a random system and then find the least-squares approximation using a deterministic QR-based method and a randomized method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic QR-based method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the algorithm on random matrices of varying dimensions, and plot the time it took to run the algorithm as a function of the input size:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph](images/2020-07-10/ls_det.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized method\n",
    "To solve the least squares problem, we also tried devising a randomized method. Given an integer $k$, this method samples $k$ Gaussian vectors $x$ and keeps the vector that best minimizes $\\lVert Ax-b \\rVert_2$. We run the algorithm on many random $A$ matrices and $b$ vectors with entries from a standard normal distribution. Unfortunately, this naive algorithm was unable to beat the deterministic one, but we hope to optimize it enough for it to be more efficient eventually. One strategy we are considering is finding more structured ways to randomly sample the $x$ vectors, instead of choosing completely arbitrarily from a standard distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph](images/2020-07-10/ls_rand.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Idea\n",
    "Use a random matrix approximation, such as the ID, and then find a least squares solution for this approximation. Testing this with the ID gives better efficency than the above randomized approach. However, it still fails to beat our current determinsitic approach.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Next?\n",
    "\n",
    "We are currently interested in randomizing the kernel mapping in kernel PCA. While we do not have time to go into detail in this presentation, here is a quick demonstration:\n",
    "\n",
    "Given the following data:\n",
    "\n",
    "![Data](../examples/kpca/fakedata.png)\n",
    "\n",
    "We want to find an embedding in which the two groups are linearly separable (and preferably well clustered). Regular PCA (linear kernel) will fail, as the polar symmetry in the data tells us that there is no direction (linear combination of the x and y axes) that accounts for significantly more variance in the data. In order to find meaningful principle components, we utilize kernel PCA, first mapping the data points into a higher dimensional space using a kernel function. Here are some embeddings of the data using various kernels and default parameters:\n",
    "\n",
    "![Embeds](../examples/kpca/kpca_embeds.png)\n",
    "\n",
    "As we can see, none of them are linearly separable. Often, we can find a better embedding by playing around with one of the parameters. Here is an animation of embeddings using the RBF/Gaussian kernel as the gamma parameter changes, $0<\\gamma<26$.\n",
    "\n",
    "![Anim](../examples/kpca/anim.gif)\n",
    "\n",
    "While some values of gamma produce good nice/separable embeddings, choosing a value without guess and check is difficult. We are now investigating a randomized method for determining this kernel mapping, which produces nice output without having to arduously check parameters:\n",
    "\n",
    "![RKPCA](../examples/kpca/rkpca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for listening!\n",
    "\n",
    "### Questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:icerm]",
   "language": "python",
   "name": "conda-env-icerm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
