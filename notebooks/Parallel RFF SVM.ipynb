{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import KernelPCA\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of Randomized Fourier features Kernels in Parallel\n",
    "\n",
    "First, let's read in MNIST Data to use as an example, and truncate to the first $1000$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = pd.read_csv('../datasets/mnist/train.csv')\n",
    "\n",
    "full_X = mnist[mnist.columns[1:]].values / 255\n",
    "full_y = mnist.label.values\n",
    "\n",
    "X = full_X[:1000]\n",
    "y = full_y[:1000]\n",
    "\n",
    "n,d = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Generation Function\n",
    "\n",
    "We use random Fourier features to emulate a Gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kernel(m=220, s=1/d):\n",
    "    b = np.random.uniform(low=0, high=2*np.pi, size=(1,m))\n",
    "    W = np.random.multivariate_normal(mean=np.zeros(d), cov=2*s*np.eye(d), size=m) # m x d\n",
    "    def ker(x, y):\n",
    "        z1 = np.cos(x @ W.T + b)\n",
    "        z2 = np.cos(y @ W.T + b)\n",
    "        return z1 @ z2.T / m\n",
    "    return ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = (1/784) * np.arange(0.1,10,0.1)#range(0.1,0.3,0.1)\n",
    "n_param = parameters.shape[0]\n",
    "m = 220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing parameters using CV: Deterministic Kernel, Series computation\n",
    "\n",
    "The goal of these experiments is to test a range of parameters values and compare the time it takes for various methods to do that. First, we test each parameter value in series using a deterministic kernel with `sklearn.model_selection.GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144.41502749999998\n",
      "[0.1275  0.21375 0.34125 0.5375  0.65625 0.725   0.7475  0.76625 0.7825\n",
      " 0.795   0.79875 0.81    0.815   0.81625 0.82375 0.83    0.8275  0.83125\n",
      " 0.8375  0.84375 0.84125 0.8425  0.8475  0.84625 0.85    0.84875 0.8475\n",
      " 0.85    0.85375 0.855   0.85625 0.85625 0.8575  0.8575  0.8575  0.85875\n",
      " 0.85875 0.85875 0.86    0.85875 0.85875 0.85875 0.86125 0.86125 0.86125\n",
      " 0.8625  0.8625  0.8625  0.865   0.86625 0.865   0.865   0.86625 0.8675\n",
      " 0.8675  0.86875 0.87    0.87125 0.87125 0.87125 0.87125 0.87    0.87\n",
      " 0.87    0.87125 0.87125 0.87375 0.87375 0.875   0.87625 0.87625 0.87625\n",
      " 0.8775  0.8775  0.8775  0.8775  0.8775  0.8775  0.87875 0.87875 0.87875\n",
      " 0.87875 0.87875 0.88    0.88    0.88125 0.88125 0.88125 0.88125 0.88125\n",
      " 0.88125 0.88125 0.88125 0.88125 0.88125 0.88125 0.88125 0.88125 0.88125]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = perf_counter()\n",
    "\n",
    "params = {'gamma': parameters}\n",
    "svc = SVC(kernel='rbf')\n",
    "clf = GridSearchCV(svc, params, cv=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(perf_counter() - start)\n",
    "print(clf.cv_results_['mean_test_score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing parameters using CV: random Fourier features, Parallel computation\n",
    "\n",
    "The random Fourier features method has another upshot: since the kernel matrix is approximated by\n",
    "$$\\hat{K} = \\frac{1}{m}z(X)z(X)^T$$,\n",
    "we are effectively approximating the kernel by matrix multiplication. Thus, we may parallelize this code, using _batch matrix multiplication_. We used `torch.bmm` to perform this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.00260990000001\n",
      "[0.12399673 0.14284594 0.18674517 0.31667298 0.44099624 0.49311725\n",
      " 0.5970826  0.65916469 0.67415371 0.68710705 0.70813439 0.7331066\n",
      " 0.74706094 0.77212279 0.74411129 0.77707652 0.77698399 0.7740922\n",
      " 0.7870487  0.78314328 0.78405726 0.78701268 0.78516632 0.79716072\n",
      " 0.80519897 0.79601248 0.79416928 0.80209423 0.7830326  0.78908645\n",
      " 0.82207877 0.8090923  0.79708579 0.80504596 0.80205189 0.80605589\n",
      " 0.81605751 0.81606933 0.81407076 0.80511511 0.8070903  0.81811972\n",
      " 0.825127   0.81513432 0.81511934 0.82706039 0.8131084  0.82810897\n",
      " 0.81502653 0.81703774 0.81309946 0.82099074 0.81108852 0.808121\n",
      " 0.8080495  0.80710528 0.83103126 0.8080708  0.81602149 0.82207588\n",
      " 0.82909814 0.82299617 0.80809157 0.82110196 0.82997664 0.82008859\n",
      " 0.81417882 0.82507626 0.81703774 0.81698963 0.82406894 0.82602021\n",
      " 0.82318467 0.82603493 0.82904425 0.80799271 0.81803613 0.80212709\n",
      " 0.81811078 0.8369776  0.83606651 0.82513305 0.8309984  0.82111984\n",
      " 0.82814842 0.82299301 0.82508809 0.82703331 0.82601416 0.82309503\n",
      " 0.81904552 0.83407922 0.83113932 0.83610858 0.82505839 0.80804977\n",
      " 0.82310423 0.81110351 0.80809157]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# X_train is train_batch x d\n",
    "# X_test is test_batch x d\n",
    "\n",
    "n_cv = 3\n",
    "\n",
    "scores = np.empty((n_cv,n_param))\n",
    "\n",
    "start = perf_counter()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = n_cv)\n",
    "for i,(train_index, test_index) in enumerate(skf.split(X,y)):\n",
    "    X_tr, X_te = X[train_index], X[test_index]\n",
    "    y_tr, y_te = y[train_index], y[test_index]\n",
    "    \n",
    "    # m x d x n_param\n",
    "    W = np.random.multivariate_normal(mean=np.zeros(d), cov=2*np.eye(d), size=(n_param, m)).transpose(1,2,0) * np.sqrt(parameters)\n",
    "\n",
    "    # n_param x m x 1\n",
    "    b = np.random.uniform(low=0, high=2*np.pi, size=(n_param,m,1))\n",
    "\n",
    "    # Wtranspose below is n_param x m x d, X_train.T is d x train_batch, their product is n_param x m x train_batch\n",
    "\n",
    "    placeholder = np.cos(np.dot(W.transpose(2,0,1), X_tr.T) + b) # n_param x m x train_batch\n",
    "\n",
    "    z11 = torch.from_numpy(placeholder.transpose(0,2,1)) # n_param x train_batch x m\n",
    "    z2 = torch.from_numpy(placeholder.transpose(0,1,2)) # n_param x m x train_batch\n",
    "\n",
    "    z12 = torch.from_numpy(np.cos(np.dot(W.transpose(2,0,1), X_te.T) + b).transpose(0,2,1)) # n_param x test_batch x m\n",
    "\n",
    "    out1 = (1/m) * np.asarray(torch.bmm(z11, z2)) # n_param x train_batch x train_batch\n",
    "    out2 = (1/m) * np.asarray(torch.bmm(z12, z2)) # n_param x test_batch x train_batch\n",
    "\n",
    "    for j in range(n_param):\n",
    "        svm = SVC(kernel='precomputed')\n",
    "        svm.fit(out1[j], y_tr)\n",
    "        scores[i,j] = svm.score(out2[j], y_te)\n",
    "\n",
    "totaltime = perf_counter() - start\n",
    "\n",
    "print(totaltime)\n",
    "print(np.mean(scores, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "randmeans = np.mean(scores,axis=0)\n",
    "detmeans = clf.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08477965018786957"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Error of results\n",
    "np.linalg.norm(randmeans-detmeans) / np.linalg.norm(detmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best random param value was the 2th best det param value\n",
      "Difference in accuracy between best random and best det is 0.044272404147781774\n"
     ]
    }
   ],
   "source": [
    "#Check where random is maximized, and identify the order of that index in detmeans\n",
    "\n",
    "random_max_idx = np.argmax(randmeans)\n",
    "\n",
    "det_val_for_best_rand = detmeans[random_max_idx]\n",
    "\n",
    "sorted_unique_det = np.unique(np.sort(detmeans))[::-1]\n",
    "\n",
    "rank = np.where(sorted_unique_det == det_val_for_best_rand)[0][0]\n",
    "\n",
    "print(f'Best random param value was the {rank}th best det param value')\n",
    "print(f'Difference in accuracy between best random and best det is {np.max(detmeans)-np.max(randmeans)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing parameters using CV: random Fourier features, Series computation\n",
    "\n",
    "We compare our previous results to computing the random Fourier features kernels in series, and we note an almost $100\\%$ speedup using the parallel method-- this may be attributed to the fact that the machine on which this code was ran had $2$ cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.71442639999998\n",
      "[0.12399673 0.12399673 0.2126913  0.27203066 0.42601851 0.47274516\n",
      " 0.58490177 0.57593402 0.65624899 0.70017107 0.71299269 0.73704777\n",
      " 0.75101736 0.76199262 0.74702546 0.76806173 0.76008027 0.77305491\n",
      " 0.77408588 0.7910435  0.78508507 0.77108288 0.78392265 0.78911643\n",
      " 0.8060141  0.80311599 0.80106614 0.802999   0.81012643 0.80005883\n",
      " 0.79907019 0.81103725 0.80712054 0.80802242 0.8180327  0.81397742\n",
      " 0.82111063 0.82111352 0.80711395 0.81605778 0.80798405 0.81405262\n",
      " 0.83013516 0.80510039 0.8319902  0.80510012 0.81612666 0.81593473\n",
      " 0.82609171 0.81505966 0.81308132 0.81901898 0.83204779 0.81702249\n",
      " 0.82107172 0.82605307 0.83112407 0.82210269 0.81603096 0.82207588\n",
      " 0.81308159 0.8151141  0.81803297 0.82602599 0.8191144  0.81008752\n",
      " 0.83710931 0.8010601  0.82007388 0.83103126 0.80997079 0.82410469\n",
      " 0.82701543 0.83500268 0.82710508 0.83103415 0.81097125 0.83402271\n",
      " 0.81914492 0.82609487 0.8219736  0.82012804 0.81612955 0.82608304\n",
      " 0.82002314 0.81507754 0.82209087 0.82107804 0.82016063 0.82013671\n",
      " 0.8240629  0.82621765 0.81802692 0.82718263 0.81809842 0.82813921\n",
      " 0.82014565 0.82412627 0.82402399]\n"
     ]
    }
   ],
   "source": [
    "# X_train is train_batch x d\n",
    "# X_test is test_batch x d\n",
    "\n",
    "n_cv = 3\n",
    "\n",
    "scores_cv_rnd_np = np.empty((n_cv,n_param))\n",
    "\n",
    "start = perf_counter()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = n_cv)\n",
    "for i,(train_index, test_index) in enumerate(skf.split(X,y)):\n",
    "    X_tr, X_te = X[train_index], X[test_index]\n",
    "    y_tr, y_te = y[train_index], y[test_index]\n",
    "    \n",
    "    for j,val in enumerate(parameters):\n",
    "        svm = SVC(kernel=generate_kernel(s=val))\n",
    "        svm.fit(X_tr, y_tr)\n",
    "        scores_cv_rnd_np[i,j] = svm.score(X_te, y_te)\n",
    "\n",
    "\n",
    "totaltime = perf_counter() - start\n",
    "\n",
    "print(totaltime)\n",
    "print(np.mean(scores_cv_rnd_np, axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
